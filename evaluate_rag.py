# -*- coding: utf-8 -*-
"""
RAG Evaluation Script using Ragas Metrics
==========================================
Evaluates the Constitutional Legal Assistant using:
- faithfulness
- answer_relevancy
- context_precision
- context_recall
- context_relevancy

USAGE:
------
1. Command line: python evaluate_rag.py path/to/questions.json
2. Environment variable: set QA_FILE_PATH=path/to/questions.json
3. Default: Place 'test_dataset.json' in same directory

JSON FORMAT:
-----------
List format: [{"question": "...", "ground_truth": "..."}, ...]
OR dict format: {"data": [...]} or {"questions": [...]}

RATE LIMITS:
-----------
- 120 second delay between questions to avoid API timeouts
- 30 second delay before evaluation starts
- 15 second initial cooldown after pipeline load
"""

import os
import sys
import json
import time
from dotenv import load_dotenv
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from ragas.llms import LangchainLLMWrapper
from ragas.embeddings import LangchainEmbeddingsWrapper
from langchain_groq import ChatGroq
from langchain_huggingface import HuggingFaceEmbeddings
import logging

# Import the RAG pipeline initialization from the main RAG module
from rag import initialize_rag_pipeline, ask

# Suppress verbose API logging
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("groq").setLevel(logging.WARNING)

load_dotenv()
model_name="Omartificial-Intelligence-Space/GATE-AraBert-v1"
# ==========================================
# ‚è±Ô∏è RATE LIMITING / DELAYS (GROQ LIMITS)
# ==========================================
RPM_LIMIT = 30
TPM_LIMIT = 6000
RPD_LIMIT = 14400
TPD_LIMIT = 500000

# Use a conservative delay to stay within RPM limits.
# Increased delays to prevent API timeouts
MIN_DELAY_SECONDS = 60.0 / RPM_LIMIT
REQUEST_DELAY_SECONDS = 60.0  # 1 minute between each question to avoid timeouts
EVALUATION_DELAY_SECONDS = 60.0  # 60 seconds before starting evaluation
INITIAL_COOLDOWN = 10.0  # 10 seconds after loading pipeline
PER_METRIC_DELAY = 60.0  # 60 seconds between evaluating each question's metrics

# ==========================================
# üìù TEST DATASET
# ==========================================
# Default test questions (used when no file is provided)
DEFAULT_TEST_QUESTIONS = [
    {
        "question": "ŸÖÿß ŸáŸä ÿ¥ÿ±Ÿàÿ∑ ÿßŸÑÿ™ÿ±ÿ¥ÿ≠ ŸÑÿ±ÿ¶ÿßÿ≥ÿ© ÿßŸÑÿ¨ŸÖŸáŸàÿ±Ÿäÿ©ÿü",
        "ground_truth": "Ÿäÿ¨ÿ® ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿßŸÑŸÖÿ±ÿ¥ÿ≠ ŸÖÿµÿ±ŸäÿßŸã ŸÖŸÜ ÿ£ÿ®ŸàŸäŸÜ ŸÖÿµÿ±ŸäŸäŸÜÿå Ÿàÿ£ŸÑÿß ÿ™ŸÉŸàŸÜ ŸÑŸá ÿ¨ŸÜÿ≥Ÿäÿ© ÿ£ÿÆÿ±Ÿâÿå Ÿàÿ£ŸÜ ŸäŸÉŸàŸÜ ŸÖÿ™ŸÖÿ™ÿπÿßŸã ÿ®ÿ≠ŸÇŸàŸÇŸá ÿßŸÑŸÖÿØŸÜŸäÿ© ŸàÿßŸÑÿ≥Ÿäÿßÿ≥Ÿäÿ©ÿå Ÿàÿ£ŸÜ ŸäŸÉŸàŸÜ ŸÇÿØ ÿ£ÿØŸâ ÿßŸÑÿÆÿØŸÖÿ© ÿßŸÑÿπÿ≥ŸÉÿ±Ÿäÿ© ÿ£Ÿà ÿ£ÿπŸÅŸä ŸÖŸÜŸáÿß ŸÇÿßŸÜŸàŸÜÿßŸãÿå Ÿàÿ£ŸÑÿß ÿ™ŸÇŸÑ ÿ≥ŸÜŸá ŸäŸàŸÖ ŸÅÿ™ÿ≠ ÿ®ÿßÿ® ÿßŸÑÿ™ÿ±ÿ¥ÿ≠ ÿπŸÜ ÿ£ÿ±ÿ®ÿπŸäŸÜ ÿ≥ŸÜÿ© ŸÖŸäŸÑÿßÿØŸäÿ©."
    },
    {
        "question": "ŸÖÿß ŸáŸä ŸÖÿØÿ© ŸàŸÑÿßŸäÿ© ÿ±ÿ¶Ÿäÿ≥ ÿßŸÑÿ¨ŸÖŸáŸàÿ±Ÿäÿ©ÿü",
        "ground_truth": "ŸÖÿØÿ© ÿßŸÑÿ±ÿ¶ÿßÿ≥ÿ© ÿ≥ÿ™ ÿ≥ŸÜŸàÿßÿ™ ŸÖŸäŸÑÿßÿØŸäÿ©ÿå ÿ™ÿ®ÿØÿ£ ŸÖŸÜ ÿßŸÑŸäŸàŸÖ ÿßŸÑÿ™ÿßŸÑŸä ŸÑÿßŸÜÿ™Ÿáÿßÿ° ŸÖÿØÿ© ÿ≥ŸÑŸÅŸáÿå ŸàŸÑÿß Ÿäÿ¨Ÿàÿ≤ ÿ•ÿπÿßÿØÿ© ÿßŸÜÿ™ÿÆÿßÿ®Ÿá ÿ•ŸÑÿß ŸÑŸÖÿ±ÿ© Ÿàÿßÿ≠ÿØÿ©."
    },
    {
        "question": "ŸÖÿß ŸáŸä ÿ≠ŸÇŸàŸÇ ÿßŸÑŸÖŸàÿßÿ∑ŸÜ ŸÅŸä ÿßŸÑÿ≠ÿµŸàŸÑ ÿπŸÑŸâ ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ÿü",
        "ground_truth": "ÿßŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ŸàÿßŸÑÿ®ŸäÿßŸÜÿßÿ™ ŸàÿßŸÑÿ•ÿ≠ÿµÿßÿ°ÿßÿ™ ŸàÿßŸÑŸàÿ´ÿßÿ¶ŸÇ ÿßŸÑÿ±ÿ≥ŸÖŸäÿ© ŸÖŸÑŸÉ ŸÑŸÑÿ¥ÿπÿ®ÿå ŸàÿßŸÑÿ•ŸÅÿµÿßÿ≠ ÿπŸÜŸáÿß ŸÖŸÜ ŸÖÿµÿßÿØÿ±Ÿáÿß ÿßŸÑŸÖÿÆÿ™ŸÑŸÅÿ© ÿ≠ŸÇ ÿ™ŸÉŸÅŸÑŸá ÿßŸÑÿØŸàŸÑÿ© ŸÑŸÉŸÑ ŸÖŸàÿßÿ∑ŸÜ."
    },
    {
        "question": "ŸÖÿß ŸáŸà ÿØŸàÿ± ŸÖÿ¨ŸÑÿ≥ ÿßŸÑÿ¥ŸäŸàÿÆÿü",
        "ground_truth": "ŸäÿÆÿ™ÿµ ŸÖÿ¨ŸÑÿ≥ ÿßŸÑÿ¥ŸäŸàÿÆ ÿ®ÿØÿ±ÿßÿ≥ÿ© ŸàÿßŸÇÿ™ÿ±ÿßÿ≠ ŸÖÿß Ÿäÿ±ÿßŸá ŸÉŸÅŸäŸÑÿßŸã ÿ®ÿØÿπŸÖ ÿßŸÑŸàÿ≠ÿØÿ© ÿßŸÑŸàÿ∑ŸÜŸäÿ© ŸàÿßŸÑÿ≥ŸÑÿßŸÖ ÿßŸÑÿßÿ¨ÿ™ŸÖÿßÿπŸä ŸàÿßŸÑÿ≠ŸÅÿßÿ∏ ÿπŸÑŸâ ÿßŸÑŸÖŸÇŸàŸÖÿßÿ™ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ© ŸÑŸÑŸÖÿ¨ÿ™ŸÖÿπÿå ŸàÿØÿ±ÿßÿ≥ÿ© ŸÖÿ¥ÿ±Ÿàÿπÿßÿ™ ÿßŸÑŸÇŸàÿßŸÜŸäŸÜ ÿßŸÑŸÖŸÉŸÖŸÑÿ© ŸÑŸÑÿØÿ≥ÿ™Ÿàÿ±."
    },
    {
        "question": "ŸÉŸäŸÅ Ÿäÿ™ŸÖ ÿ™ÿπÿØŸäŸÑ ÿßŸÑÿØÿ≥ÿ™Ÿàÿ±ÿü",
        "ground_truth": "ŸÑÿ±ÿ¶Ÿäÿ≥ ÿßŸÑÿ¨ŸÖŸáŸàÿ±Ÿäÿ© ÿ£Ÿà ŸÑÿÆŸÖÿ≥ ÿ£ÿπÿ∂ÿßÿ° ŸÖÿ¨ŸÑÿ≥ ÿßŸÑŸÜŸàÿßÿ® ÿ∑ŸÑÿ® ÿ™ÿπÿØŸäŸÑ ŸÖÿßÿØÿ© ÿ£Ÿà ÿ£ŸÉÿ´ÿ± ŸÖŸÜ ÿßŸÑÿØÿ≥ÿ™Ÿàÿ±ÿå ŸàŸäÿ¨ÿ® ÿßŸÑŸÖŸàÿßŸÅŸÇÿ© ÿπŸÑŸâ ÿßŸÑÿ™ÿπÿØŸäŸÑ ÿ®ÿ£ÿ∫ŸÑÿ®Ÿäÿ© ÿ´ŸÑÿ´Ÿä ÿ£ÿπÿ∂ÿßÿ° ÿßŸÑŸÖÿ¨ŸÑÿ≥ÿå ÿ´ŸÖ Ÿäÿπÿ±ÿ∂ ÿπŸÑŸâ ÿßŸÑÿ¥ÿπÿ® ŸÅŸä ÿßÿ≥ÿ™ŸÅÿ™ÿßÿ°."
    }
]

def load_test_questions(file_path: str):
    """Load test questions from JSON file"""
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            obj = json.load(f)

        if isinstance(obj, list):
            return obj
        if isinstance(obj, dict):
            if "data" in obj and isinstance(obj["data"], list):
                return obj["data"]
            if "questions" in obj and isinstance(obj["questions"], list):
                return obj["questions"]
        raise ValueError("Unsupported QA JSON format; expected a list or dict with 'data' or 'questions'.")
    except FileNotFoundError:
        raise FileNotFoundError(f"‚ùå QA file not found: {file_path}")
    except json.JSONDecodeError as e:
        raise ValueError(f"‚ùå Invalid JSON format in {file_path}: {e}")
    except Exception as e:
        raise Exception(f"‚ùå Error loading QA file {file_path}: {e}")


# Load QA file path from environment variable or command line
qa_file_path = os.getenv("QA_FILE_PATH")
if not qa_file_path and len(sys.argv) > 1:
    qa_file_path = sys.argv[1]

# If still not provided, try default file
if not qa_file_path:
    default_path = "test_dataset_5_questions.json"
    if os.path.exists(default_path):
        qa_file_path = default_path
        print(f"üìÇ Using default dataset: {default_path}")

if qa_file_path and os.path.exists(qa_file_path):
    print(f"üìÇ Loading questions from: {qa_file_path}")
    try:
        test_questions = load_test_questions(qa_file_path)
        print(f"‚úÖ Loaded {len(test_questions)} questions from file")
    except Exception as e:
        print(f"‚ùå Error loading file: {e}")
        print("üìù Using default inline test questions instead")
        test_questions = DEFAULT_TEST_QUESTIONS
else:
    if qa_file_path:
        print(f"‚ö†Ô∏è File not found: {qa_file_path}")
    print("üìù Using default inline test questions")
    test_questions = DEFAULT_TEST_QUESTIONS

# ==========================================
# üîÑ RUN EVALUATION
# ==========================================

def run_evaluation():
    print("="*60)
    print("üöÄ Starting RAG Evaluation with Ragas")
    print("="*60)
    
    print(f"\nüìä Configuration:")
    print(f"   Questions to evaluate: {len(test_questions)}")
    print(f"   Delay per question (generation): {REQUEST_DELAY_SECONDS}s")
    print(f"   Delay per question (evaluation): {PER_METRIC_DELAY}s")
    
    total_gen_time = len(test_questions) * REQUEST_DELAY_SECONDS / 60.0
    total_eval_time = len(test_questions) * PER_METRIC_DELAY / 60.0
    total_time = total_gen_time + total_eval_time + INITIAL_COOLDOWN / 60.0 + EVALUATION_DELAY_SECONDS / 60.0
    
    print(f"\n‚è±Ô∏è Estimated total time:")
    print(f"   Question generation: ~{total_gen_time:.1f} minutes")
    print(f"   Evaluation phase: ~{total_eval_time:.1f} minutes")
    print(f"   Total: ~{total_time:.1f} minutes ({total_time/60:.1f} hours)\n")
    
    # 1. Initialize RAG Pipeline
    print("\nüì• Loading RAG pipeline...")
    qa_chain = initialize_rag_pipeline()
    print("‚úÖ Pipeline loaded successfully")

    # Let the service cool down before starting requests
    print(f"‚è≥ Cooling down for {INITIAL_COOLDOWN} seconds...")
    time.sleep(INITIAL_COOLDOWN)
    
    # 2. Generate answers and collect context
    print("\nü§ñ Generating answers for test questions...\n")
    
    results = {
        "question": [],
        "answer": [],
        "contexts": [],
        "ground_truth": []
    }
    
    for idx, item in enumerate(test_questions, 1):
        question = item["question"]
        ground_truth = item.get("ground_truth", "")
        
        print(f"\n{'='*60}")
        print(f"[{idx}/{len(test_questions)}] Generating answer ({idx / len(test_questions) * 100:.0f}% complete)")
        print(f"{'='*60}")
        print(f"Q: {question[:80]}...")
        print(f"{'-'*60}")
        
        try:
            # Use rag.ask which returns (answer, sources)
            answer, sources = ask(question)

            # sources is a list of dicts produced by rag._docs_to_sources
            contexts = [s.get("content", "") for s in sources]

            # Store results
            results["question"].append(question)
            results["answer"].append(answer)
            results["contexts"].append(contexts)
            results["ground_truth"].append(ground_truth)

            print(f"‚úÖ Generated answer ({len(answer)} chars)")
            print(f"‚úÖ Retrieved {len(contexts)} context documents")

            # Delay between requests to avoid hitting RPM limits
            if idx < len(test_questions):
                print(f"‚è≥ Waiting {REQUEST_DELAY_SECONDS} seconds before next question...")
                time.sleep(REQUEST_DELAY_SECONDS)
            
        except Exception as e:
            print(f"‚ùå Error: {e}")
            # Add placeholder to keep dataset aligned
            results["question"].append(question)
            results["answer"].append("Error generating answer")
            results["contexts"].append([])
            results["ground_truth"].append(ground_truth)
    
    # 3. Convert to Ragas Dataset format
    print("\nüìä Creating evaluation dataset...")
    dataset = Dataset.from_dict(results)
    print(f"‚úÖ Dataset created with {len(dataset)} samples")
    
    # 4. Run Ragas Evaluation
    print("\n‚öôÔ∏è Running Ragas evaluation...")
    print("This may take a few minutes...")
    print("Using Groq API (Llama 3.1 8B Instant) for evaluation...")

    # Add a larger delay before evaluation to avoid back-to-back bursts
    print(f"‚è≥ Waiting {EVALUATION_DELAY_SECONDS} seconds before evaluation...")
    time.sleep(EVALUATION_DELAY_SECONDS)
    
    # Configure Groq LLM for evaluation (same as app_final.py)
    evaluator_llm = LangchainLLMWrapper(ChatGroq(
        model="llama-3.1-8b-instant",  # Same as app_final.py
        temperature=0.3,  # Same as app_final.py
        model_kwargs={"top_p": 0.9},  # Same as app_final.py
        max_retries=3  # Add retries for robustness
    ))
    
    # Configure embeddings (same as app_final.py)
    print("Configuring HuggingFace embeddings (same as app_final.py)...")
    evaluator_embeddings = LangchainEmbeddingsWrapper(HuggingFaceEmbeddings(
        model_name=model_name
    ))
    
    try:
        # Evaluate each question separately with delays to avoid rate limits
        print("\n‚ö†Ô∏è Evaluating each question separately with 60-second delays...")
        print(f"‚è±Ô∏è Estimated time: ~{len(results['question']) * PER_METRIC_DELAY / 60:.1f} minutes\n")
        
        all_scores = {
            "faithfulness": [],
            "answer_relevancy": [],
            "context_precision": [],
            "context_recall": []
        }
        
        for q_idx in range(len(results["question"])):
            print(f"\n{'='*60}")
            print(f"üìã Question {q_idx + 1}/{len(results['question'])} ({(q_idx + 1) / len(results['question']) * 100:.0f}% complete)")
            print(f"{'='*60}")
            print(f"Q: {results['question'][q_idx][:80]}...")
            print(f"-" * 60)
            
            # Create single-question dataset
            single_q_data = {
                "question": [results["question"][q_idx]],
                "answer": [results["answer"][q_idx]],
                "contexts": [results["contexts"][q_idx]],
                "ground_truth": [results["ground_truth"][q_idx]]
            }
            single_dataset = Dataset.from_dict(single_q_data)
            
            # Evaluate all metrics for this question
            try:
                q_result = evaluate(
                    single_dataset,
                    metrics=[faithfulness, answer_relevancy, context_precision, context_recall],
                    llm=evaluator_llm,
                    embeddings=evaluator_embeddings,
                    raise_exceptions=False
                )
                
                # Convert EvaluationResult to dict if needed
                if hasattr(q_result, 'to_pandas'):
                    # Convert to pandas and then to dict
                    result_df = q_result.to_pandas()
                    result_dict = result_df.to_dict('records')[0] if len(result_df) > 0 else {}
                elif isinstance(q_result, dict):
                    result_dict = q_result
                else:
                    # Try to access as attributes
                    result_dict = {
                        'faithfulness': getattr(q_result, 'faithfulness', 0.0),
                        'answer_relevancy': getattr(q_result, 'answer_relevancy', 0.0),
                        'context_precision': getattr(q_result, 'context_precision', 0.0),
                        'context_recall': getattr(q_result, 'context_recall', 0.0)
                    }
                
                # Extract scores (handle if they're lists or single values)
                def get_score(value):
                    if isinstance(value, list):
                        return value[0] if len(value) > 0 else 0.0
                    return float(value) if value is not None else 0.0
                
                f_score = get_score(result_dict.get('faithfulness', 0.0))
                a_score = get_score(result_dict.get('answer_relevancy', 0.0))
                cp_score = get_score(result_dict.get('context_precision', 0.0))
                cr_score = get_score(result_dict.get('context_recall', 0.0))
                
                # Display scores for this question
                print(f"\nüìä Results for Question {q_idx + 1}:")
                print(f"   Faithfulness       : {f_score:.4f}")
                print(f"   Answer Relevancy   : {a_score:.4f}")
                print(f"   Context Precision  : {cp_score:.4f}")
                print(f"   Context Recall     : {cr_score:.4f}")
                
                all_scores["faithfulness"].append(f_score)
                all_scores["answer_relevancy"].append(a_score)
                all_scores["context_precision"].append(cp_score)
                all_scores["context_recall"].append(cr_score)
                
            except Exception as e:
                print(f"\n‚ùå Error evaluating question {q_idx + 1}: {str(e)}")
                print(f"   Error type: {type(e).__name__}")
                # Print more debug info if verbose
                import traceback
                print(f"   Traceback: {traceback.format_exc()[:200]}...")
                all_scores["faithfulness"].append(0.0)
                all_scores["answer_relevancy"].append(0.0)
                all_scores["context_precision"].append(0.0)
                all_scores["context_recall"].append(0.0)
            
            # Wait between questions to avoid rate limits
            if q_idx < len(results["question"]) - 1:
                print(f"\n‚è≥ Waiting {PER_METRIC_DELAY} seconds before next question...")
                time.sleep(PER_METRIC_DELAY)
        
        # Calculate average scores
        print("\n" + "="*60)
        print("üìä CALCULATING AVERAGE SCORES")
        print("="*60)
        
        evaluation_results = {
            "faithfulness": sum(all_scores["faithfulness"]) / len(all_scores["faithfulness"]) if all_scores["faithfulness"] else 0.0,
            "answer_relevancy": sum(all_scores["answer_relevancy"]) / len(all_scores["answer_relevancy"]) if all_scores["answer_relevancy"] else 0.0,
            "context_precision": sum(all_scores["context_precision"]) / len(all_scores["context_precision"]) if all_scores["context_precision"] else 0.0,
            "context_recall": sum(all_scores["context_recall"]) / len(all_scores["context_recall"]) if all_scores["context_recall"] else 0.0
        }
        
        print("\n" + "="*60)
        print("üìà FINAL AVERAGE RESULTS")
        print("="*60)
        
        # Display average results
        for metric_name, score in evaluation_results.items():
            if isinstance(score, (int, float)):
                print(f"  {metric_name:28s}: {score:.4f}")
        
        overall_avg = sum(evaluation_results.values()) / len(evaluation_results)
        print(f"\n  {'Overall Average':28s}: {overall_avg:.4f}")
        
        # Save results to JSON
        results_file = "evaluation_results.json"
        with open(results_file, "w", encoding="utf-8") as f:
            results_dict = {
                "metrics": {k: float(v) if isinstance(v, (int, float)) else str(v) 
                           for k, v in evaluation_results.items()},
                "individual_scores": all_scores,
                "test_samples": len(dataset),
                "overall_average": overall_avg,
                "evaluation_details": {
                    "delay_per_question": f"{REQUEST_DELAY_SECONDS}s",
                    "delay_per_metric": f"{PER_METRIC_DELAY}s",
                    "model": "llama-3.1-8b-instant",
                    "embeddings": model_name
                }
            }
            json.dump(results_dict, f, ensure_ascii=False, indent=2)
        
        print(f"\nüíæ Results saved to: {results_file}")
        
        # Save individual question breakdown
        breakdown_file = "evaluation_breakdown.json"
        breakdown_data = []
        for q_idx in range(len(results["question"])):
            # Calculate average score for this question across all metrics
            question_score = (
                all_scores["faithfulness"][q_idx] +
                all_scores["answer_relevancy"][q_idx] +
                all_scores["context_precision"][q_idx] +
                all_scores["context_recall"][q_idx]
            ) / 4.0
            
            breakdown_data.append({
                "question": results["question"][q_idx],
                "ground_truth": results["ground_truth"][q_idx],
                "actual_answer": results["answer"][q_idx],
                "score": round(question_score, 4)
            })
        
        # Calculate average score of all questions
        total_avg_score = sum(item["score"] for item in breakdown_data) / len(breakdown_data) if breakdown_data else 0.0
        
        # Create simplified results structure
        simplified_results = {
            "questions": breakdown_data,
            "average_score": round(total_avg_score, 4)
        }
        
        with open(breakdown_file, "w", encoding="utf-8") as f:
            json.dump(simplified_results, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ Question breakdown saved to: {breakdown_file}")
        print(f"üìä Average score across all questions: {total_avg_score:.4f}")
        
        # Save detailed results
        detailed_file = "evaluation_detailed.json"
        with open(detailed_file, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"üíæ Detailed results saved to: {detailed_file}")
        
        print("\n" + "="*60)
        print("‚úÖ Evaluation Complete!")
        print("="*60)
        
        return evaluation_results
        
    except Exception as e:
        print(f"\n‚ùå Evaluation failed: {e}")
        print("\n‚ö†Ô∏è Troubleshooting:")
        print("   1. Check GROQ_API_KEY is set in .env file")
        print("   2. Verify you have valid Groq API credits")
        print("   3. Ensure internet connection is stable")
        print("   4. Try increasing PER_METRIC_DELAY in the script")
        print("   5. Reduce the number of test questions")
        import traceback
        traceback.print_exc()
        return None

# ==========================================
# üìä METRIC EXPLANATIONS
# ==========================================

def print_metric_explanations():
    """Print what each metric measures"""
    print("\n" + "="*60)
    print("üìñ RAGAS METRICS EXPLANATION")
    print("="*60)
    
    explanations = {
        "faithfulness": "Is the answer grounded in the context? (0-1, higher is better)\n"
                       "Measures if the answer contains only information from the retrieved context.",
        
        "answer_relevancy": "Does the answer relate to the question? (0-1, higher is better)\n"
                           "Measures how well the answer addresses the question asked.",
        
        "context_precision": "How much retrieved context was relevant? (0-1, higher is better)\n"
                            "Measures the signal-to-noise ratio in retrieved documents.",
        
        "context_recall": "Did we retrieve all needed information? (0-1, higher is better)\n"
                         "Measures if all ground truth information is in the context.",
        
        "context_relevancy": "Overall relevance of context to question (0-1, higher is better)\n"
                            "Measures how relevant the retrieved context is to the question."
    }
    
    for metric, explanation in explanations.items():
        print(f"\n{metric.upper()}:")
        print(f"  {explanation}")
    
    print("\n" + "="*60)

# ==========================================
# üéØ MAIN EXECUTION
# ==========================================

if __name__ == "__main__":
    from datetime import datetime
    
    start_time = datetime.now()
    
    print("\n" + "="*60)
    print("üéØ RAG EVALUATION SYSTEM")
    print("   Constitutional Legal Assistant - Egyptian Constitution")
    print("="*60)
    print(f"\n‚è∞ Started at: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    
    # Print what metrics mean
    print_metric_explanations()
    
    # Run evaluation
    input("\nPress ENTER to start evaluation...")
    
    results = run_evaluation()
    
    end_time = datetime.now()
    duration = end_time - start_time
    
    print("\n" + "="*60)
    print("üìä EVALUATION SUMMARY")
    print("="*60)
    print(f"‚è∞ Started:  {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"‚è∞ Finished: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"‚è±Ô∏è Duration: {duration.total_seconds() / 60:.1f} minutes")
    print(f"üìù Questions evaluated: {len(test_questions)}")
    
    if results:
        print(f"\n‚úÖ Evaluation completed successfully!")
        print(f"\nüìÇ Output files:")
        print(f"   - evaluation_results.json (average metrics & config)")
        print(f"   - evaluation_breakdown.json (per-question scores)")
        print(f"   - evaluation_detailed.json (full Q&A data)")
    else:
        print(f"\n‚ö†Ô∏è Evaluation could not be completed.")
        print(f"   Check the error messages above for troubleshooting.")
    
    print("\n" + "="*60)
